{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if str.lower(dbutils.widgets.getAll().get(\"install_ml_libs\", \"False\")) == \"true\":\n",
    "  os.system(\"pip install -qqqq lightgbm databricks-feature-store bayesian-optimization mlflow hyperopt shap databricks-automl-runtime\")\n",
    "  dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def _get_config():\n",
    "  notebook_root = os.getcwd()\n",
    "  while not notebook_root.endswith('/notebooks'):\n",
    "      parent = os.path.dirname(notebook_root)\n",
    "      if parent == notebook_root:\n",
    "          break\n",
    "      notebook_root = parent\n",
    "  notebook_root\n",
    "  json_config_path = os.path.join(notebook_root, \"./demo_setup/config.json\")\n",
    "\n",
    "  # The DABS setup job should inject the config, and set the load_config_from_file param to False\n",
    "  # If the check is absent, we'll assume the user is running the notebook interactively\n",
    "  # and load the config from the file\n",
    "  config = dbutils.widgets.getAll()\n",
    "  if str.lower(config.get(\"load_config_from_file\", \"True\")) == \"true\":\n",
    "    try:\n",
    "      with open(json_config_path, \"r\") as f:\n",
    "        config = json.load(f)\n",
    "    except:\n",
    "      print(f\"‚ùå Could not load config from {json_config_path}\")\n",
    "      raise Exception(\"Could not load config. Please ensure you have run the setup job (deploy-iops-demo-workflow) created by Databricks Asset Bundles\")\n",
    "  else:\n",
    "    with open(json_config_path, \"w\") as f:\n",
    "      json.dump(config, f)\n",
    "\n",
    "  return config\n",
    "\n",
    "config = _get_config()\n",
    "\n",
    "catalog_name = config['catalog_name']\n",
    "schema_name = config['schema_name']\n",
    "volume_name = config['volume_name']\n",
    "\n",
    "print(f\"‚úÖ Variables configured:\")\n",
    "print(f\"   üìÅ Catalog: {catalog_name}\")\n",
    "print(f\"   üìÇ Schema: {schema_name}\")\n",
    "print(f\"   üíæ Volume: {volume_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_fs_table(table_name):\n",
    "  from databricks.feature_store import FeatureStoreClient\n",
    "  fs = FeatureStoreClient()\n",
    "  try:\n",
    "    fs.drop_table(table_name)  \n",
    "  except Exception as e:\n",
    "    print(f\"Can't drop the fs table, probably doesn't exist? {e}\")\n",
    "  try:\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS `{table_name}`\")\n",
    "  except Exception as e:\n",
    "    print(f\"Can't drop the delta table, probably doesn't exist? {e}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "00.set_variables",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
