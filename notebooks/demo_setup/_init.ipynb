{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if str.lower(dbutils.widgets.getAll().get(\"install_ml_libs\", \"False\")) == \"true\":\n",
    "  os.system(\"pip install -qqqq lightgbm databricks-feature-store bayesian-optimization mlflow hyperopt shap databricks-automl-runtime\")\n",
    "  dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def _get_config():\n",
    "\n",
    "  # Config should be stored at <solution_accelerator_root>/notebooks/demo_setup/config.json\n",
    "  # We are expecting _init.ipynb to be run from somewhere within the notebooks folder\n",
    "  # Databricks will set the current working directory to currently running notebook\n",
    "  # This will traverse up the folder structure until it finds the notebooks folder\n",
    "  notebook_root = os.getcwd()\n",
    "  while not notebook_root.endswith('/notebooks'):\n",
    "      parent = os.path.dirname(notebook_root)\n",
    "      if parent == notebook_root:\n",
    "          break\n",
    "      notebook_root = parent\n",
    "  notebook_root\n",
    "  json_config_path = os.path.join(notebook_root, \"demo_setup/config.json\")\n",
    "\n",
    "  # The DABS setup job should inject the config as job params, and set the load_config_from_file param to False\n",
    "  # If this check is absent, we'll assume the user is running the notebook interactively and load the config from the file\n",
    "  config = dbutils.widgets.getAll()\n",
    "  if str.lower(config.get(\"load_config_from_file\", \"True\")) == \"true\":\n",
    "    try:\n",
    "      with open(json_config_path, \"r\") as f:\n",
    "        config = json.load(f)\n",
    "    except:\n",
    "      # If we can't load the config from the file, assume that the setup process wasn't run\n",
    "      print(f\"‚ùå Could not load config from {json_config_path}\")\n",
    "      print(f\"Make sure that you have:\")\n",
    "      print(f\"   1. Deployed this Solution Accelerator using Databricks Asset Bundles from the UI\")\n",
    "      print(f\"   2. Run the deploy-iops-demo-workflow job, which will create the config.json file\")\n",
    "      print(f\"If you are deploying the bundle from the CLI, you will need to manually configure the config.json file\")\n",
    "      print(f\"Expected paramters are:\")\n",
    "      print(f\"   1. catalog_name\")\n",
    "      print(f\"   2. schema_name\")\n",
    "      print(f\"   3. volume_name\")\n",
    "      raise Exception(\n",
    "        \"Could not load config. Please ensure you have run the setup job (deploy-iops-demo-workflow) created by Databricks Asset Bundles\")\n",
    "  else:\n",
    "    # Preserve existing keys if config file already exists\n",
    "    existing_config = {}\n",
    "    if os.path.exists(json_config_path):\n",
    "      try:\n",
    "        with open(json_config_path, \"r\") as f:\n",
    "          existing_config = json.load(f)\n",
    "        print(f\"üìÑ Found existing config file, preserving additional keys...\")\n",
    "      except:\n",
    "        print(f\"‚ö†Ô∏è Could not read existing config file, creating new one...\")\n",
    "        existing_config = {}\n",
    "    \n",
    "    # Merge widget config with existing config (widget config takes precedence for standard keys)\n",
    "    merged_config = existing_config.copy()\n",
    "    merged_config.update(config)\n",
    "    \n",
    "    with open(json_config_path, \"w\") as f:\n",
    "      json.dump(merged_config, f, indent=2)\n",
    "    \n",
    "    # Update config to use the merged version\n",
    "    config = merged_config\n",
    "\n",
    "  return config\n",
    "\n",
    "config = _get_config()\n",
    "\n",
    "catalog_name = config['catalog_name']\n",
    "schema_name = config['schema_name']\n",
    "volume_name = config['volume_name']\n",
    "\n",
    "print(f\"‚úÖ Variables configured:\")\n",
    "print(f\"   üìÅ Catalog: {catalog_name}\")\n",
    "print(f\"   üìÇ Schema: {schema_name}\")\n",
    "print(f\"   üíæ Volume: {volume_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_fs_table(table_name):\n",
    "  from databricks.feature_store import FeatureStoreClient\n",
    "  fs = FeatureStoreClient()\n",
    "  try:\n",
    "    fs.drop_table(table_name)  \n",
    "  except Exception as e:\n",
    "    print(f\"Can't drop the fs table, probably doesn't exist? {e}\")\n",
    "  try:\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS `{table_name}`\")\n",
    "  except Exception as e:\n",
    "    print(f\"Can't drop the delta table, probably doesn't exist? {e}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "00.set_variables",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
