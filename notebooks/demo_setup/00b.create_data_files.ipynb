{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%run ./_init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create files from kaggle dataset in volumes\n",
        "\n",
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "\n",
        "def create_mock_plant_equipment():\n",
        "    data = [\n",
        "        (\"Spectrophotometer\", 1, True),\n",
        "        (\"pH Meter\", 2, True),\n",
        "        (\"Oven\", 3, True),\n",
        "        (\"Remote Arm\", 4, False),\n",
        "        (\"Thermometer\", 5, None),\n",
        "    ]\n",
        "    \n",
        "    columns = [\"name\", \"id\", \"is_active\"]\n",
        "    df_equipment = spark.createDataFrame(data, columns)\n",
        "    \n",
        "    return df_equipment\n",
        "\n",
        "# Download the zip file\n",
        "url = \"https://www.kaggle.com/api/v1/datasets/download/edumagalhaes/quality-prediction-in-a-mining-process\"\n",
        "response = requests.get(url)\n",
        "zip_file = zipfile.ZipFile(io.BytesIO(response.content))\n",
        "\n",
        "# Extract the CSV file to a UC volume\n",
        "csv_filename = \"MiningProcess_Flotation_Plant_Database.csv\"\n",
        "zip_file.extract(csv_filename, f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/\")\n",
        "\n",
        "# Define the file path\n",
        "file_path = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/MiningProcess_Flotation_Plant_Database.csv\"\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "df = spark.read.format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .load(file_path)\n",
        "\n",
        "from pyspark.sql.functions import col, hour, when, concat_ws, lit, avg\n",
        "from pyspark.sql import functions as F\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "# Rename columns to remove invalid characters\n",
        "df = df.select([F.col(col).alias(col.replace(\" \", \"_\").replace(\"%\", \"Percent\")) for col in df.columns])\n",
        "\n",
        "for col in df.columns:\n",
        "    if col != \"date\":\n",
        "        df = df.withColumn(col, F.regexp_replace(col, r\",\", \".\").cast(\"double\"))\n",
        "\n",
        "flotation_columns = [\n",
        "    \"date\",\n",
        "    \"Starch_Flow\",\n",
        "    \"Amina_Flow\", \n",
        "    \"Ore_Pulp_Flow\",\n",
        "    \"Ore_Pulp_pH\", \n",
        "    \"Ore_Pulp_Density\", \n",
        "    \"Flotation_Column_01_Air_Flow\", \n",
        "    \"Flotation_Column_02_Air_Flow\",\n",
        "    \"Flotation_Column_03_Air_Flow\",\n",
        "    \"Flotation_Column_04_Air_Flow\",\n",
        "    \"Flotation_Column_05_Air_Flow\",\n",
        "    \"Flotation_Column_06_Air_Flow\",\n",
        "    \"Flotation_Column_07_Air_Flow\",\n",
        "    \"Flotation_Column_01_Level\",\n",
        "    \"Flotation_Column_02_Level\",\n",
        "    \"Flotation_Column_03_Level\",\n",
        "    \"Flotation_Column_04_Level\",\n",
        "    \"Flotation_Column_05_Level\",\n",
        "    \"Flotation_Column_06_Level\",\n",
        "    \"Flotation_Column_07_Level\",\n",
        "]\n",
        "lab_data_columns = [\n",
        "    \"date\",\n",
        "    \"Percent_Iron_Feed\",\n",
        "    \"Percent_Silica_Feed\",\n",
        "    \"Percent_Iron_Concentrate\",\n",
        "    \"Percent_Silica_Concentrate\",\n",
        "]\n",
        "df_flotation = df.select(*flotation_columns)\n",
        "df_flotation.write.format(\"parquet\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .save(f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/flotation_data/\")\n",
        "\n",
        "df_lab = df.select(*lab_data_columns)\n",
        "\n",
        "df_lab_hourly = df_lab.groupBy(\"date\").agg(\n",
        "    avg(\"Percent_Iron_Feed\").alias(\"Percent_Iron_Feed\"),\n",
        "    avg(\"Percent_Silica_Feed\").alias(\"Percent_Silica_Feed\"),\n",
        "    avg(\"Percent_Iron_Concentrate\").alias(\"Percent_Iron_Concentrate\"),\n",
        "    avg(\"Percent_Silica_Concentrate\").alias(\"Percent_Silica_Concentrate\")\n",
        ")\n",
        "\n",
        "# Mock shift operators for PII demo\n",
        "\n",
        "# 1. List of fake operator names\n",
        "operator_names = [\n",
        "    \"Alice Johnson\", \"Ben Carter\", \"Cindy Lee\", \"David Smith\", \"Emma Wright\",\n",
        "    \"Frank Miller\", \"Grace Kim\", \"Henry Jones\", \"Isla Clarke\", \"Jack White\"\n",
        "]\n",
        "\n",
        "# Broadcast the list and create a shift ID based on date and shift\n",
        "df_with_shift = df_lab_hourly.withColumn(\"shift_type\", when(\n",
        "    (hour(\"date\") >= 6) & (hour(\"date\") < 18), \"day\"\n",
        ").otherwise(\"night\"))\n",
        "\n",
        "# Create a shift identifier (e.g., \"2024-05-13_day\")\n",
        "df_with_shift = df_with_shift.withColumn(\"shift_id\",\n",
        "    concat_ws(\"_\", F.to_date(\"date\"), F.col(\"shift_type\"))\n",
        ")\n",
        "\n",
        "# Get distinct shifts\n",
        "distinct_shifts = df_with_shift.select(\"shift_id\").distinct().collect()\n",
        "\n",
        "# Assign a random operator to each shift\n",
        "shift_operator_map = {row[\"shift_id\"]: random.choice(operator_names) for row in distinct_shifts}\n",
        "\n",
        "# Convert to a DataFrame for joining\n",
        "shift_df = spark.createDataFrame(shift_operator_map.items(), [\"shift_id\", \"operator_name\"])\n",
        "\n",
        "# Join operator name back to the main DataFrame\n",
        "df_with_operator = df_with_shift.join(shift_df, on=\"shift_id\", how=\"left\").drop(\"shift_id\")\n",
        "\n",
        "df_with_operator.write.format(\"parquet\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .save(f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/lab_data_hourly/\")\n",
        "\n",
        "# Mock equipment with a None isactive field that we can use to demo expectations\n",
        "df_equipment = create_mock_plant_equipment()\n",
        "df_equipment.write.format(\"parquet\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .save(f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/equipment/\")\n",
        "\n",
        "print(\"Data files created successfully:\")\n",
        "print(f\"- Flotation data: /Volumes/{catalog_name}/{schema_name}/{volume_name}/flotation_data/\")\n",
        "print(f\"- Lab data: /Volumes/{catalog_name}/{schema_name}/{volume_name}/lab_data_hourly/\")\n",
        "print(f\"- Equipment data: /Volumes/{catalog_name}/{schema_name}/{volume_name}/equipment/\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
